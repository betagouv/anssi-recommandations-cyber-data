# anssi-recommandations-cyber-data

Une interface permettant d'Ã©valuer le bot de l'ANSSI, basÃ© sur Albert [Albert](https://github.com/betagouv/anssi-recommandations-cyber), et d'y indexer de nouveaux documents RAG.

## ğŸ—ºï¸ Diagramme des interactions entre les composants de l'application

### Interactions pour gÃ©nÃ©rer des rÃ©ponses
```mermaid
flowchart LR
  %% === Projet local ===
  subgraph Projet["anssi-recommandations-cyber-data"]
    DataSrc["donnees/QA-labelisÃ©-Question_par_guide.csv"]
    Lecteur[LecteurCSV]
    Remplisseur[RemplisseurReponses]
    ClientMQC[ClientMQCHTTP]
  end

  %% === SystÃ¨me externe (mise en Ã©vidence) ===
  subgraph Externe["anssi-recommendations-cyber (externe)"]
    MQC[/Route HTTP POST /pose_question/]
  end

  %% Flux conforme au code
  Lecteur -->| lit | DataSrc
  Lecteur -->| utilise pour chaque question| Remplisseur
  Remplisseur -->|"remplit 'RÃ©ponse Bot'"| Lecteur

  Remplisseur -->|pose_question| ClientMQC
  ClientMQC -->|POST JSON| MQC
  MQC -->|rÃ©ponse JSON| ClientMQC
  ClientMQC -->|renvoie texte| Remplisseur


  style Externe fill:#fff3cd,stroke:#f0ad4e,stroke-width:2.5px,color:#333
```

### Interactions pour Ã©valuer le dataset
```mermaid
sequenceDiagram
    participant Main as main_evalap.py
    participant Client as ClientEvalap
    participant API as Evalap API

    Main->>Client: nouveau ClientEvalap(base_url, session)
    Client-->>Main: instance ClientEvalap
    Main->>Client: ajoute_dataset(payload)
    Note over Client: payload inclut <prefixe>_<horodatage>.csv 
    Client->>API: POST /dataset
    API-->>Client: {id, name, ...}
    Client-->>Main: DatasetReponse
```

## ğŸ“¦ Comment installer ?

### Directement sur l'hÃ´te

Il faut installer deux dÃ©pendances systÃ¨mes, `python` et `uv`.
Ensuite, la premiÃ¨re fois il faut crÃ©er un environnement virtuel avec `uv venv`.

DÃ¨s lors, l'environnement est activable via `source .venv/bin/activate`.
Les dÃ©pendances dÃ©clarÃ©es sont installables via `uv sync`.

## ğŸ§ª Comment valider ?

Dans un environnement virtuel :
* lancer `mypy` pour vÃ©rifier la validitÃ© des annotations de types,
* et lancer `pytest` pour valider le comportement Ã  l'exÃ©cution.

## âš™ï¸ Comment DÃ©finir mes variables d'environnement ?

Il faut crÃ©er Ã  la racine du projet un fichier `.env`.
A minima, ce fichier devra dÃ©finir les variables dÃ©clarÃ©es dans le fichier `.env.template`.

### Authentification Evalap

Pour utiliser l'API Evalap, ajoutez votre token d'authentification au fichier `.env` :
```
EVALAP_TOKEN=votre_token_ici
```
âš ï¸ Utilisez directement la valeur du token, sans guillemets ni formatage JSON.

## ğŸ§ª GÃ©nÃ©rer les rÃ©ponses du bot pour le jeu de validation

### ğŸ’ PrÃ©requis

1. Lancer lâ€™application [anssi-recommandations-cyber](https://github.com/betagouv/anssi-recommandations-cyber).
Pour cela, nous vous recommandons de dÃ©marrer l'application dans le **conteneur** construit avec les instructions de ce dÃ©pÃ´t :  
   ```bash
   docker container run --rm -it \
    --network=host \
    --volume $(pwd):/app \
    localhost/mqc/api \
    bash -c "env \$(cat .env) python src/main.py"
    ```
   âš ï¸ Pensez Ã  complÃ©ter le fichier `.env` Ã  partir du modÃ¨le `.env.template`.

2. VÃ©rifier que lâ€™application **MQC** dÃ©marre bien en local (endpoint `/pose_question` accessible).

### â–¶ï¸ GÃ©nÃ©ration des rÃ©ponses

ExÃ©cuter la commande suivante :

```bash
uv run --env-file .env python -m /main_remplir_csv.py   --csv donnees/jointure-nom-guide.csv   --prefixe evaluation   --sortie donnees/sortie
```

- `--csv` : chemin vers le fichier CSV contenant les questions Ã  Ã©valuer.  
- `--prefixe` : prÃ©fixe utilisÃ© dans le nom du fichier de sortie.  
- `--sortie` : dossier oÃ¹ sera Ã©crit le CSV enrichi.  

Un fichier nommÃ© `evaluation_YYYY-MM-DD_H_M_S.csv` sera alors gÃ©nÃ©rÃ© dans `donnees/sortie/` avec une colonne **RÃ©ponse Bot** remplie automatiquement.

## ğŸ“Š Ã‰valuer avec Evalap

### ğŸ’ PrÃ©requis

1. Avoir dÃ©fini dans votre fichier `.env` la variable `ALBERT_CLE_API`.  
âš ï¸ Sans cette variable, Evalap ne pourra pas interroger lâ€™API Albert.
2. Disposer de [docker](https://docs.docker.com/get-docker/) et [docker compose](https://docs.docker.com/compose/install/) installÃ©s sur votre machine.  

### â–¶ï¸ Lancer Evalap

Depuis la racine du projet, exÃ©cuter :

```bash
docker compose -f evalap-compose.yml up -d
```

### âœ… VÃ©rifications

Sâ€™assurer que les conteneurs dÃ©marrent correctement et que lâ€™interface Evalap est accessible :
- l'IHM de l'API est accessible Ã  l'adresse : http://localhost:8000/docs
- l'IHM de l'application web est accessible Ã  l'adresse : http://localhost:8501

Si les urls ne semblent pas accessibles, vÃ©rifier quâ€™aucun conflit de port nâ€™apparaÃ®t dans les logs.

### ğŸ“Š Ã‰valuation

Une fois l'application lancÃ©e, pour Ã©valuer un jeu de donnÃ©es :

1) Ajouter un dataset. ExÃ©cutez :
```bash
uv run --env-file .env python src/main_evalap.py --csv donnees/sortie/evaluation_2025-09-30_17-20-16.csv --nom nom_dataset
```
Le chemin passÃ© Ã  `--csv` est celui gÃ©nÃ©rÃ© Ã  lâ€™Ã©tape [Â« â–¶ï¸ GÃ©nÃ©ration des rÃ©ponses Â»](#%EF%B8%8F-g%C3%A9n%C3%A9ration-des-r%C3%A9ponses).

## ğŸ”§ Ajouter une nouvelle mÃ©trique personnalisÃ©e Evalap

### ğŸ“ CrÃ©er une nouvelle mÃ©trique

1. CrÃ©er le fichier de mÃ©trique dans `src/metriques_personnalisees_evalap/ma_metrique.py`, en se basant sur la [documentation Evalap](https://evalap.etalab.gouv.fr/doc/fr/docs/developer-guide/adding-a-new-metric) :

### ğŸš€ DÃ©ployer la mÃ©trique

```bash
docker compose -f evalap-compose.yml build evalap
docker compose -f evalap-compose.yml up -d
```

### âœ… VÃ©rifier que la mÃ©trique est disponible

```bash
curl -X GET "http://localhost:8000/v1/metrics" -H "accept: application/json" | jq '.[] | select(.name == "ma_metrique")'
```

Remplacez `ma_metrique` par le nom de votre mÃ©trique (ex: `exact_match`).