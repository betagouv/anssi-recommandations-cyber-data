name: Automatisation des calculs de métrique avec Evalap
permissions: {}

on:
  push:
    branches: [main, interroge-MQC-parralele]

jobs:
  collecte_reponses:
    name: Collecte les réponses aux questions données auprès de l'API MQC
    runs-on: ubuntu-latest

    env:
      MQC_HOTE: demo-mes-questions-cyber.cleverapps.io
      MQC_API_PREFIXE_ROUTE: "api"
      MQC_ROUTE_POSE_QUESTION: "pose_question"
      MQC_DELAI_ATTENTE_MAXIMUM: 180.0
      MQC_PORT: 443

    outputs:
      questions_avec_reponses: ${{steps.collecte_reponses_mqc.outputs.fichier_genere}}

    steps:
      - name: Cloner le dépôt Git
        uses: actions/checkout@v4

      - name: Installer Python
        uses: actions/setup-python@v5
        with:
          python-version-file: "pyproject.toml"

      - name: Installer uv
        uses: astral-sh/setup-uv@v6

      - name: Installer les dépendances
        run: uv sync

      - name: Appelle MQC pour générer les réponses pour les questions posées
        id: collecte_reponses_mqc
        run: |
          uv run python src/main_remplir_csv.py \
            --csv donnees/questions_avec_verite_terrain.csv \
            --prefixe test_github_actions \
            --sortie donnees/sortie \
            --taille-lot 10
          echo "fichier_genere=$(ls donnees/sortie)" >> "$GITHUB_OUTPUT"

      - name: Upload du résultat
        uses: actions/upload-artifact@v4
        with:
          name: sortie
          path: donnees/sortie/${{steps.collecte_reponses_mqc.outputs.fichier_genere}}

  evalue_reponses:
    name: Evalue les réponses de MQC aux questions.
    environment: DEMO
    runs-on: ubuntu-latest
    needs: collecte_reponses

    outputs:
      id_experience: ${{steps.evaluation_metriques.outputs.id_experience}}

    env:
      ALBERT_CLE_API: ${{ secrets.ALBERT_CLE_API }}
      ALBERT_URL: https://albert.api.etalab.gouv.fr/v1
      EVALAP_URL: https://app-1a24ae36-a96f-4f6d-9cf1-f1af734405fc.cleverapps.io/v1
      EVALAP_TOKEN: ${{ secrets.EVALAP_TOKEN }}

    steps:
      - name: Cloner le dépôt Git
        uses: actions/checkout@v4

      - name: Installer Python
        uses: actions/setup-python@v5
        with:
          python-version-file: "pyproject.toml"

      - name: Installer uv
        uses: astral-sh/setup-uv@v6

      - name: Installer les dépendances
        run: uv sync

      - name: Télécharge l'artifact
        uses: actions/download-artifact@v4
        with:
          name: sortie
          path: donnees/sortie


      - name: Adapte le fichier des réponses et le soumet à Evalap
        id: evaluation_metriques
        run: |
          id_experience=$(uv run python src/main_evalap.py \
            --csv donnees/sortie/${{needs.collecte_reponses.outputs.questions_avec_reponses}} \
            --nom github_actions_$(date +%Y%m%d_%H%M%S) | tail -1)
          echo "ID d'expérience récupéré: '$id_experience'"
          if [ -z "$id_experience" ]; then
            echo "Erreur: ID d'expérience vide"
            exit 1
          fi
          echo "id_experience=$id_experience" >> "$GITHUB_OUTPUT"

  consigne_evaluation:
    name: Consigne l'évaluation
    environment: DEMO
    runs-on: ubuntu-latest
    needs: evalue_reponses

    steps:
      - name: Cloner le dépôt Git
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Cloner l'intégralité du dépôt, pour ne pas avoir de « shallow repo » rejeté par Clever

      - name: Installer la CLI Clever Cloud
        shell: bash
        run: npm install -g clever-tools

      - name: Déployer en DÉMO
        env:
          CLEVER_SECRET: ${{ secrets.CLEVER_CLOUD_SECRET }}
          CLEVER_TOKEN: ${{ secrets.CLEVER_CLOUD_TOKEN }}
          ID_APP: ${{ secrets.CLEVER_CLOUD_ID_APP }}
          ID_ORGANISATION: ${{ secrets.CLEVER_CLOUD_ID_ORGANISATION }}
        run: |
          ID_EXP="${{needs.evalue_reponses.outputs.id_experience}}"
          if [ -z "$ID_EXP" ]; then
            echo "Erreur: ID d'expérience non défini depuis l'étape précédente"
            exit 1
          fi
          RUN_COMMAND="uv run python src/consignateur_evaluation.py --id-experience $ID_EXP"
          echo "Commande à exécuter: $RUN_COMMAND"
          clever link -o="$ID_ORGANISATION" "$ID_APP"
          clever env set CC_RUN_COMMAND "$RUN_COMMAND"
          clever deploy --force --same-commit-policy=rebuild
