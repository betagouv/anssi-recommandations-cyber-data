name: Automatisation des calculs de métrique avec Evalap
permissions: {}

on:
  push:
    branches: [main,  ecrit-evaluation-metabase]

jobs:
  collecte_reponses:
    name: Collecte les réponses aux questions données auprès de l'API MQC
    runs-on: ubuntu-latest

    env:
      MQC_HOTE: demo-mes-questions-cyber.cleverapps.io
      MQC_API_PREFIXE_ROUTE: "api"
      MQC_ROUTE_POSE_QUESTION: "pose_question"
      MQC_DELAI_ATTENTE_MAXIMUM: 19.0
      MQC_PORT: 443

    outputs:
      questions_avec_reponses: ${{steps.collecte_reponses_mqc.outputs.fichier_genere}}

    steps:
      - name: Cloner le dépôt Git
        uses: actions/checkout@v4

      - name: Installer Python
        uses: actions/setup-python@v5
        with:
          python-version-file: "pyproject.toml"

      - name: Installer uv
        uses: astral-sh/setup-uv@v6

      - name: Installer les dépendances
        run: uv sync

      - name: Appelle MQC pour générer les réponses pour les questions posées
        id: collecte_reponses_mqc
        run: |
          uv run python src/main_remplir_csv.py \
            --csv donnees/questions_avec_verite_terrain.csv \
            --prefixe test_github_actions \
            --sortie donnees/sortie
          echo "fichier_genere=$(ls donnees/sortie)" >> "$GITHUB_OUTPUT"

      - name: Upload du résultat
        uses: actions/upload-artifact@v4
        with:
          name: sortie
          path: donnees/sortie/${{steps.collecte_reponses_mqc.outputs.fichier_genere}}

  evalue_reponses:
    name: Evalue les réponses de MQC aux questions.
    environment: DEMO
    runs-on: ubuntu-latest
    needs: collecte_reponses

    env:
      ALBERT_CLE_API: ${{ secrets.ALBERT_CLE_API }}
      ALBERT_URL: https://albert.api.etalab.gouv.fr/v1
      EVALAP_URL: https://app-1a24ae36-a96f-4f6d-9cf1-f1af734405fc.cleverapps.io/v1
      EVALAP_TOKEN: ${{ secrets.EVALAP_TOKEN }}

    outputs:
      fichier_evaluation: ${{steps.evaluation_metriques.outputs.fichier_genere}}

    steps:
      - name: Cloner le dépôt Git
        uses: actions/checkout@v4

      - name: Installer Python
        uses: actions/setup-python@v5
        with:
          python-version-file: "pyproject.toml"

      - name: Installer uv
        uses: astral-sh/setup-uv@v6

      - name: Installer les dépendances
        run: uv sync

      - name: Télécharge l'artifact
        uses: actions/download-artifact@v4
        with:
          name: sortie
          path: donnees/sortie


      - name: Adapte le fichier des réponses et le soumet à Evalap
        id: evaluation_metriques
        run: |
          uv run python src/main_evalap.py \
            --csv donnees/sortie/${{needs.collecte_reponses.outputs.questions_avec_reponses}} \
            --nom github_actions_$(date +%Y%m%d_%H%M%S)
          echo "fichier_genere=$(ls donnees/resultats_evaluations | tail -1)" >> "$GITHUB_OUTPUT"

      - name: Upload des résultats d'évaluation
        uses: actions/upload-artifact@v4
        with:
          name: resultats_evaluation
          path: donnees/resultats_evaluations/${{steps.evaluation_metriques.outputs.fichier_genere}}

  consigne_evaluation:
    name: Consigne l'évaluation
    environment: DEMO
    runs-on: ubuntu-latest
    needs: evalue_reponses

    env:
      DB_HOST: ${{ secrets.DB_HOST}}
      DB_PORT: ${{ secrets.DB_PORT }}
      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
      DB_NAME: ${{ secrets.DB_NAME }}
      DB_USER: ${{ secrets.DB_USER }}

    steps:
      - name: Cloner le dépôt Git
        uses: actions/checkout@v4

      - name: Installer Python
        uses: actions/setup-python@v5
        with:
          python-version-file: "pyproject.toml"

      - name: Installer uv
        uses: astral-sh/setup-uv@v6

      - name: Installer les dépendances
        run: uv sync

      - name: Télécharge l'artifact des résultats d'évaluation
        uses: actions/download-artifact@v4
        with:
          name: resultats_evaluation
          path: donnees/resultats_evaluations

      - name: Consigne l'évaluation
        run: |
          uv run python src/consignateur_evaluation.py \
            --csv donnees/resultats_evaluations/${{needs.evalue_reponses.outputs.fichier_evaluation}}